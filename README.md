# Wind Turbine Data Processing Pipeline

## Description

This project develops a data processing pipeline for a renewable energy company. It focuses on processing and storing data from wind turbines.

## Repository Structure

This repository is organised into several key folders:

- `data`: Contains all the data files used or generated by the project. (In a normal situation, add the content to .gitignore, it is being left as it is because it's an assessment.)

- `drivers`: Necessary drivers for the project (postgres driver)

- `src`: Source code of the project.
  - `tests`: Files to test the correct operation of the code.
  
## Installation and Setup

### Important Disclaimer

Before proceeding with the implementation, it is important to ensure that the server or machine where the code will be executed is configured in the UTC timezone. This is necessary because Pyspark will apply timezone changes based on the machine's timezone setting. Failure to observe this detail may result in data discrepancies.

### Prerequisites

- PostgreSQL must be installed on the machine, as it will be used to store the processed data.

### Database Creation

From the root of the project, execute the following command to create the database:

   ```bash
   psql postgres -f create_database.sql
   ```

   This command uses the create_database.sql script to set up the necessary database for the project.

### Configuration File Setup

Inside the src folder, you will find a file named _config_example.py. Copy this file and rename the copy to config.py.

   ```bash
   cp src/_config_example.py src/config.py
   ```

Edit the config.py file to include your specific database credentials:

- Replace the placeholders for the database username and password with your actual database credentials.
- Adjust any other configurations as needed for your environment.

### Installing Dependencies

Install the necessary dependencies for the project. In your Python environment, run:

   ```bash
   pip install -r requirements.txt
   ```

## Usage

### Running the Project

To run the project, follow these steps:

1. Change directory to the `src` folder:

   ```bash
   cd src
   ```

2. From the src directory, execute the main script:

   ```bash
    python main.py
   ```

   This will start the data processing pipeline using the configurations and scripts within the src folder.

### Running the Tests

To run the tests for the project:

1. Execute the pytest command:

   ```bash
    pytest
   ```

    This will run all the tests written for the project, ensuring that each component of the pipeline functions as expected.

### Format the code

This repository uses the black library for its formatting.

1. Execute the black command from root folder:

   ```bash
    black .
   ```

## Assumptions

- **Outlier Detection Across All Turbines:** It is assumed that the turbines maintain a similar distribution of values over time. Therefore, for detecting outliers, all values are used (without separating by turbine). This approach eliminates the need to calculate values in conjunction with all data. When only a new day's data is added, instead of having 24 values, there will be 24 * NÂº of turbines values.

- **Null Value Completion:** It is assumed that if a null value is found between two values, A and B, the null value(s) will be interpolated to be between these two values. This assumption is used for filling in missing data points.

- **Consistent Data Entry:** It is assumed that all turbines should have data entries for every hour. Therefore, if a row is missing, it is created with null values. Subsequently, the logic for null value completion, as explained above, is applied to these rows.

- **Handling of Last Turbine Value:** The last value we have for a turbine is assumed to be the final value in the CSV file. No further values will be created beyond this point using the interpolation method mentioned above, as it is assumed that the turbine has ceased operation.

- **Server Time Zone:** It is crucially assumed, and important to note, that the server where the code is executed will have its timezone set to UTC. This is because Pyspark utilizes the machine's timezone setting for certain calculations, and discrepancies can occur if this is not adhered to.

- **Data Batch Duration Assumption:** It is assumed that the data will always be provided in batches representing whole days, such as 24, 48, 72 hours, etc.

## Solution Design

The following steps outline the pipeline's workflow:

### 1. Timestamp Check

- The pipeline first checks the database for the latest timestamps recorded for each turbine.
- It then reads the CSV files containing turbine data and filters out the information. For each turbine, only data that is newer than the last recorded timestamp in the database is retained.
- If no new information is available, the program terminates.

### 2. Data Cleaning

- The initial step in data cleaning involves checking for all expected timestamps between the first and last ones for each turbine. If any are missing, they are generated with null values.
- Next, the data is checked for outliers using the Interquartile Range (IQR) method. Detected outliers are treated as null values.
- The pipeline identifies groups of consecutive null values for each turbine.
- These null values are filled, ensuring that the imputed values fall within the range of the previous and next non-null values. If a null value is at the beginning of the data, it takes the value of the next non-null entry; if it's at the end, it takes the value of the previous non-null entry.

### 3. Calculating Summary Statistics

- The pipeline calculates summary statistics for each turbine and day, including minimum, maximum, average, and standard deviation of power output.
- These statistics are stored in a new DataFrame for further analysis.

### 4. Anomaly Detection

- Anomalies are identified as values that deviate more than 2 standard deviations from the mean.
- A new column is added to the main DataFrame to flag these anomalies.

### 5. Data Storage

- Finally, the processed data is stored in two PostgreSQL tables, as set up in the initial project setup.
